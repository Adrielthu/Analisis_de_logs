{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74e96047-a9f8-438e-8131-33d59e47c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc  # Garbage Collector - Para gestión manual de memoria y liberación de recursos\n",
    "import re  # Expresiones Regulares - Para manipulación y búsqueda de patrones en texto\n",
    "import time  # Funciones de tiempo - Para medición de ejecución, delays y timestamps\n",
    "import logging  # Sistema de logging - Para registro y monitorización de eventos de la aplicación\n",
    "import ipaddress  # Manipulación de direcciones IP - Para validación y operaciones con IPs\n",
    "import numpy as np  # NumPy - Librería fundamental para computación científica con arrays\n",
    "import pandas as pd  # Pandas - Manipulación y análisis de datos estructurados (DataFrames)\n",
    "from tqdm import tqdm  # Barras de progreso - Para mostrar progreso en loops iterativos\n",
    "import geoip2.database  # GeoIP2 - Para geolocalización de direcciones IP\n",
    "from datetime import datetime  # Manipulación de fechas y horas - Para manejo de timestamps\n",
    "from sqlalchemy import create_engine  # SQLAlchemy - ORM y conexión a bases de datos SQL\n",
    "from data.config import DB_CONNECTIONS  # Configuración personalizada - Conexiones a BD del proyecto\n",
    "from ua_parser import user_agent_parser  # User Agent Parser - Para analizar strings de user agents\n",
    "\n",
    "from Modulos.update_dimensions_table import actualizarTablaDimension\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from data.config import DB_CONNECTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4cde66-d3b6-4296-b491-910145c8e850",
   "metadata": {},
   "source": [
    "# ----------------------------- SCRIPT HIPER-MEGA-ULTRA-OPTIMIZADO ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1de21f0-fa34-4439-8c3f-922a8f46001c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 02:40:47,386 - INFO - ================================================================================\n",
      "2025-09-19 02:40:47,388 - INFO -                   PROCESAMIENTO DE LOGS\n",
      "2025-09-19 02:40:47,390 - INFO - ================================================================================\n",
      "2025-09-19 02:40:47,392 - INFO - Archivo: ../logs/access.log\n",
      "2025-09-19 02:40:47,394 - INFO - Tamaño de chunk: 100,000\n",
      "2025-09-19 02:40:47,396 - INFO - Memoria en uso: 166.3 MB\n",
      "2025-09-19 02:40:48,497 - INFO - Procesados 1 chunks | 99,997 líneas | 1.10s último chunk\n",
      "2025-09-19 02:40:48,498 - INFO - Velocidad promedio: 90874 líneas/segundo\n",
      "2025-09-19 02:40:57,261 - INFO - Procesados 10 chunks | 999,985 líneas | 0.82s último chunk\n",
      "2025-09-19 02:40:57,262 - INFO - Velocidad promedio: 101371 líneas/segundo\n",
      "2025-09-19 02:41:08,390 - INFO - Procesados 20 chunks | 1,999,964 líneas | 0.92s último chunk\n",
      "2025-09-19 02:41:08,391 - INFO - Velocidad promedio: 95266 líneas/segundo\n",
      "2025-09-19 02:41:18,442 - INFO - Procesados 30 chunks | 2,999,916 líneas | 0.83s último chunk\n",
      "2025-09-19 02:41:18,443 - INFO - Velocidad promedio: 96629 líneas/segundo\n",
      "2025-09-19 02:41:28,407 - INFO - Procesados 40 chunks | 3,999,887 líneas | 0.86s último chunk\n",
      "2025-09-19 02:41:28,408 - INFO - Velocidad promedio: 97533 líneas/segundo\n",
      "2025-09-19 02:41:38,473 - INFO - Procesados 50 chunks | 4,999,841 líneas | 0.89s último chunk\n",
      "2025-09-19 02:41:38,474 - INFO - Velocidad promedio: 97889 líneas/segundo\n",
      "2025-09-19 02:41:48,624 - INFO - Procesados 60 chunks | 5,999,817 líneas | 0.96s último chunk\n",
      "2025-09-19 02:41:48,625 - INFO - Velocidad promedio: 97993 líneas/segundo\n",
      "2025-09-19 02:41:58,747 - INFO - Procesados 70 chunks | 6,999,783 líneas | 0.91s último chunk\n",
      "2025-09-19 02:41:58,750 - INFO - Velocidad promedio: 98101 líneas/segundo\n",
      "2025-09-19 02:42:08,631 - INFO - Procesados 80 chunks | 7,999,762 líneas | 0.82s último chunk\n",
      "2025-09-19 02:42:08,631 - INFO - Velocidad promedio: 98478 líneas/segundo\n",
      "2025-09-19 02:42:18,507 - INFO - Procesados 90 chunks | 8,999,732 líneas | 0.82s último chunk\n",
      "2025-09-19 02:42:18,508 - INFO - Velocidad promedio: 98779 líneas/segundo\n",
      "2025-09-19 02:42:29,240 - INFO - Procesados 100 chunks | 9,999,719 líneas | 1.06s último chunk\n",
      "2025-09-19 02:42:29,240 - INFO - Velocidad promedio: 98188 líneas/segundo\n",
      "2025-09-19 02:42:34,496 - INFO - Fin del archivo alcanzado\n",
      "2025-09-19 02:42:34,496 - INFO - Combinando todos los chunks...\n",
      "2025-09-19 02:42:35,566 - INFO - ============================================================\n",
      "2025-09-19 02:42:35,567 - INFO -                   PROCESAMIENTO COMPLETADO\n",
      "2025-09-19 02:42:35,568 - INFO - ============================================================\n",
      "2025-09-19 02:42:35,570 - INFO - Tiempo de combinación: 1.00s\n",
      "2025-09-19 02:42:35,571 - INFO - Tiempo total: 1.80 minutos\n",
      "2025-09-19 02:42:35,572 - INFO - Total de registros: 10,364,866\n",
      "2025-09-19 02:42:42,564 - INFO - Memoria del DataFrame: 2581.41 MB\n",
      "2025-09-19 02:42:42,565 - INFO - Memoria en uso: 4190.7 MB\n",
      "2025-09-19 02:42:43,291 - INFO - ================================================================================\n",
      "2025-09-19 02:42:43,291 - INFO -                   CREACIÓN DE DIMENSIONES Y TABLA DE HECHOS\n",
      "2025-09-19 02:42:43,293 - INFO - ================================================================================\n",
      "2025-09-19 02:42:43,294 - INFO - ────────────────────────────────────────────────────────────\n",
      "2025-09-19 02:42:43,296 - INFO -                   Procesando dimensión NAVEGADOR\n",
      "2025-09-19 02:42:43,297 - INFO - ────────────────────────────────────────────────────────────\n",
      "2025-09-19 02:42:49,799 - INFO - Parseando 28,350 user agents únicos...\n",
      "Parsing UA: 100%|██████████| 28350/28350 [00:38<00:00, 740.57it/s]\n",
      "2025-09-19 02:43:36,100 - INFO - Dimensión navegador creada: 155 registros únicos\n",
      "2025-09-19 02:43:36,809 - INFO - Dimensión NAVEGADOR completada en 53.51s\n",
      "2025-09-19 02:43:36,895 - INFO - ────────────────────────────────────────────────────────────\n",
      "2025-09-19 02:43:36,896 - INFO -                   Procesando dimensión DISPOSITIVO\n",
      "2025-09-19 02:43:36,898 - INFO - ────────────────────────────────────────────────────────────\n",
      "2025-09-19 02:43:36,930 - INFO - Aplicando limpieza de modelos...\n",
      "2025-09-19 02:43:36,935 - INFO - Dimensión dispositivo creada: 2973 dispositivos únicos\n",
      "2025-09-19 02:43:37,035 - INFO - Dimensión DISPOSITIVO completada en 0.14s\n",
      "2025-09-19 02:43:37,104 - INFO - ────────────────────────────────────────────────────────────\n",
      "2025-09-19 02:43:37,105 - INFO -                   Procesando dimensión ESTADO SERVER\n",
      "2025-09-19 02:43:37,106 - INFO - ────────────────────────────────────────────────────────────\n",
      "2025-09-19 02:43:37,222 - INFO - Dimensión estado server creada: 15 códigos únicos\n",
      "2025-09-19 02:43:37,307 - INFO - Dimensión ESTADO SERVER completada en 0.20s\n",
      "2025-09-19 02:43:37,308 - INFO - ────────────────────────────────────────────────────────────\n",
      "2025-09-19 02:43:37,309 - INFO -                   Procesando dimensión UBICACIÓN\n",
      "2025-09-19 02:43:37,310 - INFO - ────────────────────────────────────────────────────────────\n",
      "2025-09-19 02:43:45,670 - INFO - Geolocalizando 258,445 IPs únicas...\n",
      "Geolocalizando: 100%|██████████| 258445/258445 [00:19<00:00, 13573.84it/s]\n",
      "2025-09-19 02:46:03,596 - INFO - Dimensión ubicación creada: 1771 ubicaciones únicas\n",
      "2025-09-19 02:46:05,074 - INFO - Dimensión UBICACIÓN completada en 2.46 minutos\n",
      "2025-09-19 02:46:06,159 - INFO - ────────────────────────────────────────────────────────────\n",
      "2025-09-19 02:46:06,161 - INFO -                   Procesando dimensión TIEMPO\n",
      "2025-09-19 02:46:06,163 - INFO - ────────────────────────────────────────────────────────────\n",
      "2025-09-19 02:46:11,681 - INFO - Dimensión tiempo creada: 5 fechas únicas\n",
      "2025-09-19 02:46:11,898 - INFO - Dimensión TIEMPO completada en 5.73s\n",
      "2025-09-19 02:46:11,900 - INFO - Liberando memoria de columnas no necesarias...\n",
      "2025-09-19 02:46:53,882 - INFO - Memoria liberada: 4343.57 MB\n",
      "2025-09-19 02:46:53,883 - INFO - ────────────────────────────────────────────────────────────\n",
      "2025-09-19 02:46:53,884 - INFO -                   Creando TABLA DE HECHOS\n",
      "2025-09-19 02:46:53,885 - INFO - ────────────────────────────────────────────────────────────\n",
      "2025-09-19 02:46:53,886 - INFO - Realizando joins con dimensiones...\n",
      "2025-09-19 02:49:49,288 - INFO - Calculando métricas agregadas...\n",
      "2025-09-19 02:50:07,653 - INFO - Tabla de hechos creada: 103,008 registros agregados\n",
      "2025-09-19 02:51:06,793 - INFO - TABLA DE HECHOS completada en 252.91s\n",
      "2025-09-19 02:51:06,795 - INFO - Liberando memoria...\n",
      "2025-09-19 02:51:06,900 - INFO - Memoria en uso: 3932.2 MB\n",
      "2025-09-19 02:51:06,901 - INFO - ================================================================================\n",
      "2025-09-19 02:51:06,909 - INFO -                     PROCESO ETL FINALIZADO\n",
      "2025-09-19 02:51:06,910 - INFO - ================================================================================\n",
      "2025-09-19 02:52:13,060 - INFO - Memoria final liberada\n"
     ]
    }
   ],
   "source": [
    "# logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def liberar_memoria(*variables):\n",
    "    \"\"\"Libera memoria de variables\"\"\"\n",
    "    for var in variables:\n",
    "        if var is not None:\n",
    "            del var\n",
    "    gc.collect()\n",
    "    \n",
    "def mostrar_uso_memoria():\n",
    "    \"\"\"Muestra uso actual de memoria\"\"\"\n",
    "    import psutil\n",
    "    import os\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    logger.info(f\"Memoria en uso: {memory_mb:.1f} MB\")\n",
    "\n",
    "def procesar_archivo_log(filepath, chunk_size=100000):\n",
    "    \"\"\"Procesa archivo de log completo sin límite de líneas\"\"\"\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(\"                  PROCESAMIENTO DE LOGS\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(f\"Archivo: {filepath}\")\n",
    "    logger.info(f\"Tamaño de chunk: {chunk_size:,}\")\n",
    "    mostrar_uso_memoria()\n",
    "    \n",
    "    colz = [\"IP\", \"timestamp\", \"status code\", \"bytes sent\", \"user agent\"]\n",
    "    all_chunks = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        lines_processed = 0\n",
    "        chunk_count = 0\n",
    "        \n",
    "        while True:\n",
    "            li = []\n",
    "            chunk_start_time = time.time()\n",
    "            \n",
    "            # Procesar chunk\n",
    "            for _ in range(chunk_size):\n",
    "                try:\n",
    "                    x = f.readline()\n",
    "                    if not x:  # EOF\n",
    "                        break\n",
    "                    log = x.split(\" \")\n",
    "                    if len(log) < 12:\n",
    "                        continue\n",
    "                    li.append([log[0], log[3].strip('['), int(log[8]), int(log[9]), ' '.join(log[11:-1]).strip('\"')])\n",
    "                    lines_processed += 1\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            if not li:  # No más datos\n",
    "                logger.info(\"Fin del archivo alcanzado\")\n",
    "                break\n",
    "            \n",
    "            # Crear DataFrame del chunk\n",
    "            chunk_df = pd.DataFrame(li, columns=colz)\n",
    "            chunk_df[\"timestamp\"] = pd.to_datetime(chunk_df[\"timestamp\"], format='%d/%b/%Y:%H:%M:%S')\n",
    "            all_chunks.append(chunk_df)\n",
    "            \n",
    "            chunk_count += 1\n",
    "            chunk_time = time.time() - chunk_start_time\n",
    "            \n",
    "            # Log cada 10 chunks para reducir verbosidad\n",
    "            if chunk_count % 10 == 0 or chunk_count == 1:\n",
    "                logger.info(f\"Procesados {chunk_count} chunks | {lines_processed:,} líneas | {chunk_time:.2f}s último chunk\")\n",
    "                \n",
    "                # Estimación de velocidad\n",
    "                if lines_processed > 0:\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    speed = lines_processed / elapsed_time\n",
    "                    logger.info(f\"Velocidad promedio: {speed:.0f} líneas/segundo\")\n",
    "            \n",
    "            # Liberar memoria del chunk individual\n",
    "            liberar_memoria(li, chunk_df)\n",
    "    \n",
    "    # Combinar todos los chunks\n",
    "    if all_chunks:\n",
    "        logger.info(\"Combinando todos los chunks...\")\n",
    "        combine_start = time.time()\n",
    "        logs_df = pd.concat(all_chunks, ignore_index=True)\n",
    "        combine_time = time.time() - combine_start\n",
    "        \n",
    "        # Liberar memoria de chunks individuales\n",
    "        liberar_memoria(all_chunks)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"                  PROCESAMIENTO COMPLETADO\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(f\"Tiempo de combinación: {combine_time:.2f}s\")\n",
    "        logger.info(f\"Tiempo total: {total_time/60:.2f} minutos\")\n",
    "        logger.info(f\"Total de registros: {len(logs_df):,}\")\n",
    "        logger.info(f\"Memoria del DataFrame: {logs_df.memory_usage(deep=True).sum()/1024**2:.2f} MB\")\n",
    "        mostrar_uso_memoria()\n",
    "        \n",
    "        return logs_df\n",
    "    else:\n",
    "        logger.warning(\"No se procesaron datos válidos\")\n",
    "        return pd.DataFrame(columns=colz)\n",
    "\n",
    "def parse_ua(ua):\n",
    "    \"\"\"Parse para navegador y dispositivo\"\"\"\n",
    "    try:\n",
    "        parsed = user_agent_parser.Parse(str(ua))\n",
    "        return {\n",
    "            \"nombre\": parsed.get(\"user_agent\", {}).get(\"family\") or \"desconocido\",\n",
    "            \"so\": parsed.get(\"os\", {}).get(\"family\") or \"desconocido\",\n",
    "            \"marca\": parsed.get(\"device\", {}).get(\"brand\") or \"desconocido\",\n",
    "            \"modelo\": parsed.get(\"device\", {}).get(\"model\") or \"desconocido\"\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"nombre\": \"desconocido\", \"so\": \"desconocido\", \"marca\": \"desconocido\", \"modelo\": \"desconocido\"}\n",
    "\n",
    "def cargar_DW(logs_df, engine_cubo):\n",
    "    \"\"\"Carga todas las dimensiones y tabla de hechos\"\"\"\n",
    "    \n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(\"                  CARGA DE DIMENSIONES Y TABLA DE HECHOS\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # Dimensión d_navegador\n",
    "    # -----------------------------------------------------------\n",
    "    logger.info(\"─\" * 60)\n",
    "    logger.info(\"                  Procesando dimensión NAVEGADOR\")\n",
    "    logger.info(\"─\" * 60)\n",
    "    navegador_start = time.time()\n",
    "    \n",
    "    unique_uas = logs_df[\"user agent\"].dropna().astype(str).unique()\n",
    "    logger.info(f\"Parseando {len(unique_uas):,} user agents únicos...\")\n",
    "\n",
    "    parsed_list = [parse_ua(ua) | {\"user agent\": ua} for ua in tqdm(unique_uas, desc=\"Parsing UA\")]\n",
    "    ua_df = pd.DataFrame(parsed_list)\n",
    "\n",
    "    # merge con logs_df para traer columnas ya parseadas\n",
    "    logs_df = logs_df.merge(ua_df, on=\"user agent\", how=\"left\")\n",
    "    \n",
    "    d_navegador = ua_df[[\"nombre\"]].drop_duplicates().reset_index(drop=True)\n",
    "    d_navegador.insert(0, \"id_navegador\", range(1, len(d_navegador) + 1))\n",
    "    \n",
    "    logger.info(f\"Dimensión navegador creada: {len(d_navegador)} registros únicos\")\n",
    "    d_navegador = actualizarTablaDimension(engine_cubo, \"d_navegador\", d_navegador, pk=\"id_navegador\")\n",
    "    \n",
    "    navegador_time = time.time() - navegador_start\n",
    "    logger.info(f\"Dimensión NAVEGADOR completada en {navegador_time:.2f}s\")\n",
    "    \n",
    "    # Liberar memoria\n",
    "    liberar_memoria(unique_uas, parsed_list)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Dimensión d_dispositivo\n",
    "    # -----------------------------------------------------------\n",
    "    logger.info(\"─\" * 60)\n",
    "    logger.info(\"                  Procesando dimensión DISPOSITIVO\")\n",
    "    logger.info(\"─\" * 60)\n",
    "    dispositivo_start = time.time()\n",
    "    \n",
    "    d_dispositivo = ua_df[[\"so\", \"marca\", \"modelo\"]].drop_duplicates().reset_index(drop=True)\n",
    "    d_dispositivo.insert(0, \"id_dispositivo\", range(1, len(d_dispositivo) + 1))\n",
    "    \n",
    "    # Limpiar modelos\n",
    "    logger.info(\"Aplicando limpieza de modelos...\")\n",
    "    d_dispositivo['modelo'] = d_dispositivo['modelo'].apply(lambda x: x.split(' ', 1)[1] if x.lower().startswith('rola ') else x)\n",
    "    d_dispositivo['modelo'] = d_dispositivo['modelo'].apply(lambda x: x.split('Build')[0].strip() if 'Build' in x else x)\n",
    "    \n",
    "    logger.info(f\"Dimensión dispositivo creada: {len(d_dispositivo)} dispositivos únicos\")\n",
    "    d_dispositivo = actualizarTablaDimension(engine_cubo, \"d_dispositivo\", d_dispositivo, pk=\"id_dispositivo\")\n",
    "    \n",
    "    dispositivo_time = time.time() - dispositivo_start\n",
    "    logger.info(f\"Dimensión DISPOSITIVO completada en {dispositivo_time:.2f}s\")\n",
    "    \n",
    "    # Liberar ua_df ya que no se necesita más\n",
    "    liberar_memoria(ua_df)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Dimensión d_estadoserver\n",
    "    # -----------------------------------------------------------\n",
    "    logger.info(\"─\" * 60)\n",
    "    logger.info(\"                  Procesando dimensión ESTADO SERVER\")\n",
    "    logger.info(\"─\" * 60)\n",
    "    estado_start = time.time()\n",
    "    \n",
    "    # Diccionario de status codes\n",
    "    status_descriptions = {\n",
    "        # Respuestas informativas (100-199)\n",
    "        100: \"Continue\",\n",
    "        101: \"Switching Protocols\",\n",
    "        102: \"Processing\",\n",
    "        103: \"Early Hints\",\n",
    "        \n",
    "        # Respuestas exitosas (200-299)\n",
    "        200: \"OK\",\n",
    "        201: \"Created\",\n",
    "        202: \"Accepted\",\n",
    "        203: \"Non-Authoritative Information\",\n",
    "        204: \"No Content\",\n",
    "        205: \"Reset Content\",\n",
    "        206: \"Partial Content\",\n",
    "        207: \"Multi-Status\",\n",
    "        208: \"Already Reported\",\n",
    "        226: \"IM Used\",\n",
    "        \n",
    "        # Redirecciones (300-399)\n",
    "        300: \"Multiple Choices\",\n",
    "        301: \"Moved Permanently\",\n",
    "        302: \"Found\",\n",
    "        303: \"See Other\",\n",
    "        304: \"Not Modified\",\n",
    "        305: \"Use Proxy\",\n",
    "        307: \"Temporary Redirect\",\n",
    "        308: \"Permanent Redirect\",\n",
    "        \n",
    "        # Errores del cliente (400-499)\n",
    "        400: \"Bad Request\",\n",
    "        401: \"Unauthorized\",\n",
    "        402: \"Payment Required\",\n",
    "        403: \"Forbidden\",\n",
    "        404: \"Not Found\",\n",
    "        405: \"Method Not Allowed\",\n",
    "        406: \"Not Acceptable\",\n",
    "        407: \"Proxy Authentication Required\",\n",
    "        408: \"Request Timeout\",\n",
    "        409: \"Conflict\",\n",
    "        410: \"Gone\",\n",
    "        411: \"Length Required\",\n",
    "        412: \"Precondition Failed\",\n",
    "        413: \"Payload Too Large\",\n",
    "        414: \"URI Too Long\",\n",
    "        415: \"Unsupported Media Type\",\n",
    "        416: \"Range Not Satisfiable\",\n",
    "        417: \"Expectation Failed\",\n",
    "        418: \"I'm a teapot\",\n",
    "        421: \"Misdirected Request\",\n",
    "        422: \"Unprocessable Entity\",\n",
    "        423: \"Locked\",\n",
    "        424: \"Failed Dependency\",\n",
    "        425: \"Too Early\",\n",
    "        426: \"Upgrade Required\",\n",
    "        428: \"Precondition Required\",\n",
    "        429: \"Too Many Requests\",\n",
    "        431: \"Request Header Fields Too Large\",\n",
    "        451: \"Unavailable For Legal Reasons\",\n",
    "        499: \"Client Closed Request\",\n",
    "        \n",
    "        # Errores del servidor (500-599)\n",
    "        500: \"Internal Server Error\",\n",
    "        501: \"Not Implemented\",\n",
    "        502: \"Bad Gateway\",\n",
    "        503: \"Service Unavailable\",\n",
    "        504: \"Gateway Timeout\",\n",
    "        505: \"HTTP Version Not Supported\",\n",
    "        506: \"Variant Also Negotiates\",\n",
    "        507: \"Insufficient Storage\",\n",
    "        508: \"Loop Detected\",\n",
    "        510: \"Not Extended\",\n",
    "        511: \"Network Authentication Required\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    d_estadoserver = pd.DataFrame(logs_df[\"status code\"].unique(), columns=[\"id_estadoserver\"])\n",
    "    d_estadoserver[\"descripcion\"] = d_estadoserver[\"id_estadoserver\"].map(status_descriptions).fillna(\"Unknown\")\n",
    "    d_estadoserver[\"tipo\"] = pd.cut(d_estadoserver[\"id_estadoserver\"], \n",
    "                            bins=[0, 200, 300, 400, 500, 600, float('inf')],\n",
    "                            labels=[\"Informativo\", \"Exitoso\", \"Redirección\", \n",
    "                                   \"Error Cliente\", \"Error Servidor\", \"Otro\"],\n",
    "                            right=False)\n",
    "    \n",
    "    d_estadoserver = d_estadoserver.sort_values(\"id_estadoserver\").reset_index(drop=True)\n",
    "    \n",
    "    logger.info(f\"Dimensión estado server creada: {len(d_estadoserver)} códigos únicos\")\n",
    "    d_estadoserver = actualizarTablaDimension(engine_cubo, \"d_estadoserver\", d_estadoserver, pk=\"id_estadoserver\")\n",
    "    \n",
    "    estado_time = time.time() - estado_start\n",
    "    logger.info(f\"Dimensión ESTADO SERVER completada en {estado_time:.2f}s\")\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # Dimensión d_ubicacion\n",
    "    # -----------------------------------------------------------\n",
    "    logger.info(\"─\" * 60)\n",
    "    logger.info(\"                  Procesando dimensión UBICACIÓN\")\n",
    "    logger.info(\"─\" * 60)\n",
    "    ubicacion_start = time.time()\n",
    "    \n",
    "    logs_df['IP'] = logs_df['IP'].astype(str).str.strip()\n",
    "    unique_ips = logs_df['IP'].unique()\n",
    "    logger.info(f\"Geolocalizando {len(unique_ips):,} IPs únicas...\")\n",
    "    \n",
    "    reader = geoip2.database.Reader('../data/GeoLite2-City.mmdb')\n",
    "    \n",
    "    geo_data = []\n",
    "    for ip in tqdm(unique_ips, desc=\"Geolocalizando\"):\n",
    "        try:\n",
    "            response = reader.city(ip)\n",
    "            geo_data.append({\n",
    "                'IP': ip,\n",
    "                'continente': response.continent.name or 'Privada/Local',\n",
    "                'pais': response.country.name or 'Privada/Local',\n",
    "                'ciudad': response.city.name or 'Privada/Local',\n",
    "                'latitud': response.location.latitude,\n",
    "                'longitud': response.location.longitude\n",
    "            })\n",
    "        except:\n",
    "            geo_data.append({\n",
    "                'IP': ip,\n",
    "                'continente': 'Privada/Local',\n",
    "                'pais': 'Privada/Local',\n",
    "                'ciudad': 'Privada/Local',\n",
    "                'latitud': None,\n",
    "                'longitud': None\n",
    "            })\n",
    "    \n",
    "    reader.close()\n",
    "    \n",
    "    # Merge con logs_df\n",
    "    geo_df = pd.DataFrame(geo_data)\n",
    "    logs_df = logs_df.merge(geo_df, on='IP')\n",
    "    \n",
    "    d_ubicacion = logs_df[[\"continente\", \"pais\", \"ciudad\", \"latitud\", \"longitud\"]].drop_duplicates().reset_index(drop=True)\n",
    "    d_ubicacion.insert(0, \"id_ubicacion\", range(1, len(d_ubicacion) + 1))\n",
    "    \n",
    "    logger.info(f\"Dimensión ubicación creada: {len(d_ubicacion)} ubicaciones únicas\")\n",
    "    d_ubicacion = actualizarTablaDimension(engine_cubo, \"d_ubicacion\", d_ubicacion, pk=\"id_ubicacion\")\n",
    "    \n",
    "    ubicacion_time = time.time() - ubicacion_start\n",
    "    logger.info(f\"Dimensión UBICACIÓN completada en {ubicacion_time/60:.2f} minutos\")\n",
    "    \n",
    "    # Liberar memoria de geolocalización\n",
    "    liberar_memoria(unique_ips, geo_data, geo_df)\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # Dimensión d_tiempo\n",
    "    # -----------------------------------------------------------\n",
    "    logger.info(\"─\" * 60)\n",
    "    logger.info(\"                  Procesando dimensión TIEMPO\")\n",
    "    logger.info(\"─\" * 60)\n",
    "    tiempo_start = time.time()\n",
    "    \n",
    "    logs_df[\"mes\"] = logs_df[\"timestamp\"].dt.month\n",
    "    logs_df[\"dia\"] = logs_df[\"timestamp\"].dt.day\n",
    "    \n",
    "    d_tiempo = logs_df[[\"mes\", \"dia\"]].drop_duplicates().sort_values([\"mes\", \"dia\"]).reset_index(drop=True)\n",
    "    d_tiempo.insert(0, \"id_tiempo\", range(1, len(d_tiempo) + 1))\n",
    "    \n",
    "    logger.info(f\"Dimensión tiempo creada: {len(d_tiempo)} fechas únicas\")\n",
    "    d_tiempo = actualizarTablaDimension(engine_cubo, 'd_tiempo', d_tiempo, pk='id_tiempo')\n",
    "    \n",
    "    tiempo_time = time.time() - tiempo_start\n",
    "    logger.info(f\"Dimensión TIEMPO completada en {tiempo_time:.2f}s\")\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # Liberación de memoria\n",
    "    # -----------------------------------------------------------\n",
    "    logger.info(\"Liberando memoria de columnas no necesarias...\")\n",
    "    memory_before = logs_df.memory_usage(deep=True).sum() / 1024**2\n",
    "    logs_df = logs_df.drop(columns=[\"IP\", \"timestamp\", \"continente\", \"pais\", \"ciudad\", \"user agent\"])\n",
    "    memory_after = logs_df.memory_usage(deep=True).sum() / 1024**2\n",
    "    logger.info(f\"Memoria liberada: {memory_before - memory_after:.2f} MB\")\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # Tabla de Hechos\n",
    "    # -----------------------------------------------------------\n",
    "    logger.info(\"─\" * 60)\n",
    "    logger.info(\"                  Creando TABLA DE HECHOS\")\n",
    "    logger.info(\"─\" * 60)\n",
    "    hechos_start = time.time()\n",
    "    \n",
    "    # Merges\n",
    "    logger.info(\"Realizando joins con dimensiones...\")\n",
    "    logs_df = logs_df.merge(d_navegador, on=[\"nombre\"])\n",
    "    logs_df = logs_df.merge(d_dispositivo, on=[\"so\", \"marca\", \"modelo\"])\n",
    "    logs_df = logs_df.merge(d_ubicacion, on=[\"latitud\", \"longitud\"])\n",
    "    logs_df = logs_df.merge(d_tiempo, on=[\"mes\", \"dia\"])\n",
    "    logs_df = logs_df.rename(columns={\"status code\": \"id_estadoserver\"})\n",
    "    \n",
    "    logger.info(\"Calculando métricas agregadas...\")\n",
    "    hechos = logs_df.groupby(\n",
    "        [\"id_tiempo\", \"id_ubicacion\", \"id_dispositivo\", \"id_navegador\", \"id_estadoserver\"]\n",
    "    ).agg(\n",
    "        cant_pais_dia=(\"pais\", \"nunique\"),\n",
    "        cant_navegador_dia=(\"id_navegador\", \"nunique\"),\n",
    "        cant_dispositivo_dia=(\"id_dispositivo\", \"nunique\"),\n",
    "        cant_bytes_dispositivo=(\"bytes sent\", \"sum\"),\n",
    "        cant_estadoserver_dia=(\"id_estadoserver\", \"nunique\")\n",
    "    ).reset_index()\n",
    "    \n",
    "    hechos[\"id_hecho\"] = hechos.index + 1\n",
    "    \n",
    "    logger.info(f\"Tabla de hechos creada: {len(hechos):,} registros agregados\")\n",
    "    actualizarTablaDimension(engine_cubo, 'factable', hechos, pk='id_hecho')\n",
    "    \n",
    "    hechos_time = time.time() - hechos_start\n",
    "    logger.info(f\"Tabla de hechos cargada en {hechos_time:.2f}s\")\n",
    "    \n",
    "    # Liberación final de memoria\n",
    "    logger.info(\"Liberando memoria...\")\n",
    "    liberar_memoria(d_navegador, d_dispositivo, d_estadoserver, d_ubicacion, d_tiempo, logs_df, hechos)\n",
    "    mostrar_uso_memoria()\n",
    "    \n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(\"                    PROCESO ETL FINALIZADO\")\n",
    "    logger.info(\"=\" * 80)\n",
    "\n",
    "# Establecer conexiones a las bases de datos\n",
    "engine_cubo = create_engine(DB_CONNECTIONS[\"engine_cubo\"])\n",
    "\n",
    "# Ejecución en Jupyter\n",
    "filepath = \"../logs/access_ssl_20230404.log\"\n",
    "\n",
    "# Procesar archivo\n",
    "logs_df = procesar_archivo_log(filepath, chunk_size=100000)\n",
    "\n",
    "# Crear dimensiones y hechos\n",
    "cargar_DW(logs_df, engine_cubo)\n",
    "\n",
    "# Liberación final completa\n",
    "logger.info(\"Memoria final liberada\")\n",
    "liberar_memoria(logs_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
